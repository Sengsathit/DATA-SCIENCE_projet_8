{"cells":[{"cell_type":"markdown","id":"766ff278","metadata":{},"source":["<img src=\"https://raw.githubusercontent.com/Sengsathit/OCR_data_scientist_assets/main/header_fruits.png\" alt=\"Alternative text\" />"]},{"cell_type":"markdown","id":"288e1d05","metadata":{},"source":["# Introduction"]},{"cell_type":"markdown","id":"9a924561","metadata":{},"source":["Ce notebook a pour objectif la mise en place d'une chaîne de traitement dans un environnement Big Data, en s'appuyant sur PySpark et une architecture cloud AWS EMR. Le projet vise à compléter les travaux d'un alternant en développant des briques essentielles pour la gestion de données volumineuses liées à la classification d'images de fruits.\n","\n","L'accent sera mis sur la scalabilité des traitements et la conformité au RGPD en veillant à l'utilisation de serveurs européens."]},{"cell_type":"markdown","id":"27ac9832","metadata":{},"source":["# Import des librairies"]},{"cell_type":"code","execution_count":null,"id":"ad562eab","metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import io\n","import os\n","import tensorflow as tf\n","from PIL import Image\n","from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n","from tensorflow.keras.preprocessing.image import img_to_array\n","from tensorflow.keras import Model\n","from pyspark.sql.functions import col, pandas_udf, PandasUDFType, element_at, split\n","from pyspark.sql import functions as F\n","from pyspark.sql.types import ArrayType, DoubleType\n","from pyspark.sql import Row\n","from pyspark.ml.feature import PCA as SparkPCA\n","from pyspark.ml.linalg import Vectors, VectorUDT, Vector"]},{"cell_type":"markdown","id":"83663cbd","metadata":{},"source":["# Définition des PATH pour charger les images et enregistrer les résultats"]},{"cell_type":"markdown","id":"5d81ad8a","metadata":{},"source":["Nous accédons à nos données sur **S3** comme si elles étaient stockées localement, en définissant les chemins comme suit :"]},{"cell_type":"code","execution_count":null,"id":"46be859d","metadata":{"trusted":true},"outputs":[],"source":["PATH = 's3://ocr-p9'                        # l'URI du bucket du projet\n","PATH_data = PATH+'/data'                    # l'URI du répertoire des images à traiter\n","PATH_results = PATH+'/results'              # l'URI des features extraites des images et sauvegardées au format parquet\n","print(f\"PATH :\\t\\t{PATH}\")\n","print(f\"PATH_data :\\t{PATH_data}\")\n","print(f\"PATH_results :\\t{PATH_results}\")"]},{"cell_type":"markdown","id":"cf883c20","metadata":{},"source":["# Traitement des données"]},{"cell_type":"markdown","id":"2ffe93f5","metadata":{},"source":["## Chargement des données"]},{"cell_type":"markdown","id":"f87826f8","metadata":{},"source":["Nous utilisons Spark pour lire des fichiers binaires (comme des images) et les convertir en un format compatible avec le traitement distribué à grande échelle. \n","\n","Cela produit un DataFrame Spark contenant à la fois les métadonnées des fichiers et leur contenu binaire, permettant ainsi l'ingestion de grandes quantités d'images dans un environnement distribué."]},{"cell_type":"code","execution_count":null,"id":"7e4b319a","metadata":{"trusted":true},"outputs":[],"source":["# Lecture des images\n","images = ( spark.read.format(\"binaryFile\")\n","\n","  # Filtre pour ne lire que les fichiers avec l'extension .jpg\n","  .option(\"pathGlobFilter\", \"*.jpg\")\n","\n","  # Recherche récursive dans les sous-dossiers\n","  .option(\"recursiveFileLookup\", \"true\")\n","\n","  # Charge les fichiers à partir du S3\n","  .load(PATH_data)\n",")"]},{"cell_type":"code","execution_count":null,"id":"16bfeb4d","metadata":{"trusted":true},"outputs":[],"source":["images.show(5)"]},{"cell_type":"code","execution_count":null,"id":"a52ab808","metadata":{"trusted":true},"outputs":[],"source":["# Ajoute une nouvelle colonne 'label' au DataFrame 'images'.\n","images = images.withColumn('label', element_at(split(images['path'], '/'),-2))\n","\n","print(images.printSchema())\n","print(images.select('path','label').show(5,False))"]},{"cell_type":"markdown","id":"8f15b199","metadata":{},"source":["## Préparation du modèle"]},{"cell_type":"markdown","id":"19be0ed7","metadata":{},"source":["Nous préparons un modèle de deep learning en utilisant l'architecture MobileNetV2 pré-entraînée sur le dataset ImageNet.\n","\n","Nous allons utiliser ce modèle pour extraire des caractéristiques (features) des images, plutôt que de les classer. Le modèle prendra en entrée des images de taille 224x224 pixels et produira des features issues de l'avant-dernière couche du réseau. Ces caractéristiques pourront ensuite être utilisées pour d'autres tâches, comme la classification personnalisée sur un jeu de données spécifique ou d'autres types de traitement.\n","\n","L'objectif ici est donc d'exploiter les capacités du modèle pré-entraîné pour obtenir des représentations d'images riches, sans utiliser la couche de classification finale dédiée aux 1000 classes d'ImageNet."]},{"cell_type":"code","execution_count":null,"id":"ec7c7165","metadata":{"trusted":true},"outputs":[],"source":["# Initialisation du modèle MobileNetV2 avec des poids pré-entraînés sur ImageNet\n","model = MobileNetV2(\n","    weights='imagenet',         # Chargement des poids pré-entraînés sur le dataset ImageNet.\n","    include_top=True,           # Inclut la couche fully-connected de sortie utilisée pour la classification sur 1000 classes.\n","    input_shape=(224, 224, 3)   # Définit la taille d'entrée des images en 224x224 pixels avec 3 canaux (images en couleur).\n",")\n"]},{"cell_type":"code","execution_count":null,"id":"1b9bc650","metadata":{"trusted":true},"outputs":[],"source":["# Création d'un nouveau modèle basé sur MobileNetV2 en modifiant la sortie du modèle\n","new_model = Model(\n","    inputs=model.input,                 # On conserve les mêmes entrées que le modèle MobileNetV2 d'origine.\n","    outputs=model.layers[-2].output     # La sortie est modifiée pour être la sortie de l'avant-dernière couche (au lieu de la dernière couche de classification).\n",")  \n"]},{"cell_type":"code","execution_count":null,"id":"a0d497f2","metadata":{"trusted":true},"outputs":[],"source":["# Diffusion des poids du modèle MobileNetV2 dans l'environnement Spark\n","model_weights = sc.broadcast(new_model.get_weights())"]},{"cell_type":"code","execution_count":null,"id":"1bc0bf14","metadata":{"trusted":true},"outputs":[],"source":["new_model.summary()"]},{"cell_type":"code","execution_count":null,"id":"be8fe2b9","metadata":{"trusted":true},"outputs":[],"source":["def get_weighted_pretrained_model():\n","    \"\"\"\n","    Crée et retourne un modèle MobileNetV2 avec la couche fully-connected finale retirée.\n","    \n","    Utilisation dans un environnement Spark distribué :\n","    \n","    - Sur le driver (master) : Les poids du modèle sont initialisés puis diffusés (broadcast) à tous les workers.\n","    - Sur chaque worker (node) : Cette fonction instancie une copie locale du modèle MobileNetV2, applique les poids diffusés depuis le master (grâce à l'objet broadcast), et prépare le modèle pour traiter des images à l'échelle distribuée.\n","    \n","    Le modèle est configuré pour l'extraction de features (sans réentraînement) car toutes les couches sont gelées.\n","\n","    Returns:\n","        new_model: Un modèle MobileNetV2 modifié sans la couche de classification finale et avec les poids pré-entraînés appliqués.\n","    \"\"\"\n","\n","    # Crée le modèle MobileNetV2 avec les poids pré-entraînés sur ImageNet\n","    model = MobileNetV2(\n","        weights='imagenet',\n","        include_top=True,           # Inclut la couche fully-connected originale (que nous allons retirer ensuite)\n","        input_shape=(224, 224, 3)   # Taille d'entrée fixée à 224x224 avec 3 canaux\n","    )\n","    \n","    # Marquer toutes les couches du modèle comme non entraînables afin d'empêcher la mise à jour des poids des couches lors de l'entraînement ou de l'utilisation du modèle\n","    for layer in model.layers:\n","        layer.trainable = False\n","    \n","    # Créer un nouveau modèle avec l'avant-dernière couche comme sortie (pour extraire des features)\n","    new_model = Model(\n","        inputs=model.input,\n","        outputs=model.layers[-2].output  # Retrait de la dernière couche de classification\n","    )\n","    \n","    # Appliquer les poids diffusés (broadcasted) aux couches du modèle\n","    new_model.set_weights(model_weights.value)\n","    \n","    # Retourner le modèle modifié\n","    return new_model"]},{"cell_type":"markdown","id":"c032f135","metadata":{},"source":["## Définition du processus de chargement des images et application de leur featurisation à travers l'utilisation de pandas UDF"]},{"cell_type":"code","execution_count":null,"id":"933100cf","metadata":{"scrolled":true,"trusted":true},"outputs":[],"source":["def preprocess_image(content):\n","    \"\"\"\n","    Prend en entrée le contenu binaire d'une image, la redimensionne à 224x224 pixels, la convertit en tableau NumPy et applique un prétraitement spécifique au modèle.\n","\n","    Args:\n","    content (bytes): Contenu binaire de l'image.\n","\n","    Returns:\n","    numpy.ndarray: Image prétraitée prête pour l'entrée dans un modèle de deep learning.\n","    \"\"\"\n","\n","    # Ouvre l'image à partir du contenu binaire et la redimensionne à 224x224 pixels\n","    img = Image.open(io.BytesIO(content)).resize([224, 224])\n","    \n","    # Convertit l'image redimensionnée en tableau NumPy\n","    arr = img_to_array(img)\n","    \n","    # Applique une transformation de prétraitement pour adapter l'image à l'entrée du modèle\n","    return preprocess_input(arr)\n","\n","\n","\n","def extract_features(model, content_series):\n","    \"\"\"\n","    Transforme une série de contenus d'images en une série de vecteurs de caractéristiques.\n","\n","    Args:\n","    model (tensorflow.keras.Model): Le modèle de deep learning utilisé pour extraire les caractéristiques.\n","    content_series (pd.Series): Série Pandas contenant des images (sous forme de contenus binaires).\n","\n","    Returns:\n","    pd.Series: Série Pandas contenant des vecteurs de caractéristiques aplatis (output du modèle).\n","    \"\"\"\n","\n","    # Applique la fonction preprocess_image à chaque élément de la série, puis empile les résultats dans un tableau NumPy\n","    input = np.stack(content_series.map(preprocess_image))\n","\n","    # Fait une prédiction sur l'ensemble des images prétraitées, c'est une extraction des features dans notre cas\n","    preds = model.predict(input)\n","\n","    # Aplati chaque prédiction pour transformer la sortie du modèle en un vecteur 1D\n","    output = [p.flatten() for p in preds]\n","\n","    # Retourne les vecteurs aplatis sous forme de série Pandas\n","    return pd.Series(output)\n","\n","\n","\n","@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n","def extract_features_udf(content_series_iter):\n","    \"\"\"\n","    Applique un modèle de deep learning pour extraire des caractéristiques à partir d'un flux de contenus d'images en utilisant une Pandas UDF. Le modèle est initialisé une seule fois par partition.\n","\n","    Args:\n","    content_series_iter (Iterator[pd.Series]): Un itérateur sur des séries Pandas de contenus d'images binaires.\n","\n","    Yields:\n","    Iterator[array<float>]: Un itérateur sur des tableaux de floats représentant les vecteurs de caractéristiques extraits pour chaque image.\n","    \"\"\"\n","\n","    # Charge un modèle pré-entraîné\n","    model = get_weighted_pretrained_model()\n","\n","    # Boucle sur chaque série d'images (une série correspond à une partition de données)\n","    for content_series in content_series_iter:\n","        # Applique la fonction extract_features pour extraire les caractéristiques des images de la série\n","        yield extract_features(model, content_series)"]},{"cell_type":"markdown","id":"f23206e8","metadata":{},"source":["## Exécutions des actions d'extractions de features"]},{"cell_type":"code","execution_count":null,"id":"5e07fd68","metadata":{"trusted":true},"outputs":[],"source":["# Distribution des données sur 24 partitions, ce qui permet à Spark d'exécuter le traitement en parallèle sur ces partitions. \n","# Le nombre de partitions est ajusté pour optimiser l'utilisation des ressources disponibles (CPU, mémoire)\n","data_features = images.repartition(24).select(\n","    col(\"path\"),                                        # Sélectionne la colonne 'path' qui contient les chemins des images\n","    col(\"label\"),                                       # Sélectionne la colonne 'label', représentant l'étiquette de chaque image\n","    extract_features_udf(\"content\").alias(\"features\")   # Applique la fonction UDF pour extraire les features\n",")"]},{"cell_type":"code","execution_count":null,"id":"7c53ddd5","metadata":{"trusted":true},"outputs":[],"source":["( \n","    data_features\n","    .write                  # Déclenche une action qui oblige Spark à exécuter toutes les transformations planifiées précédemment\n","    .mode(\"overwrite\")      # Utilise le mode 'overwrite', ce qui signifie que les fichiers existants seront écrasés s'ils existent\n","    .parquet(PATH_results)  # Enregistre les données au format Parquet dans le chemin spécifié\n",")  \n","\n","# Note : Spark utilise une exécution paresseuse (lazy execution). Cela signifie que les transformations comme repartition ou select\n","# ne sont pas exécutées immédiatement. Cependant, l'appel à une action, comme write ici, déclenche l'exécution de toutes les étapes\n","# de transformation précédentes pour écrire les données sur le disque."]},{"cell_type":"markdown","id":"1fe01b72","metadata":{},"source":["# Chargement des données enregistrées et validation du résultat"]},{"cell_type":"code","execution_count":null,"id":"db18a784","metadata":{"trusted":true},"outputs":[],"source":["df = pd.read_parquet(PATH_results, engine='pyarrow')"]},{"cell_type":"code","execution_count":null,"id":"d750d2a8","metadata":{"scrolled":true,"trusted":true},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"id":"b29205ff","metadata":{"scrolled":true,"trusted":true},"outputs":[],"source":["df.loc[0,'features'].shape"]},{"cell_type":"code","execution_count":null,"id":"4fba6455","metadata":{"scrolled":false,"trusted":true},"outputs":[],"source":["df.shape"]},{"cell_type":"markdown","id":"87c8dc85","metadata":{},"source":["# ACP"]},{"cell_type":"markdown","id":"2cca9bce","metadata":{},"source":["Nous appliquons une réduction de dimension sur les features extraites des images à l'aide de l'Analyse en Composantes Principales (ACP), puis nous sauvegardons ces données réduites dans un fichier CSV pour une utilisation future.\n","\n","L'objectif ici est de réaliser une réduction de dimension dans un contexte distribué avec Spark, ce qui permet de en manipuler un grand volume de données de manière parallèle et distribuée"]},{"cell_type":"markdown","id":"2dbfa6be","metadata":{},"source":["## Fonctions réutilisables"]},{"cell_type":"code","execution_count":null,"id":"847dbc32","metadata":{},"outputs":[],"source":["def convert_to_vector(arr):\n","    \"\"\"\n","    Convertit un tableau NumPy en un vecteur dense Spark.\n","\n","    Args:\n","    arr (numpy.ndarray): Un tableau NumPy contenant les features extraites d'une image.\n","\n","    Returns:\n","    pyspark.ml.linalg.DenseVector: Un vecteur dense compatible avec Spark MLlib pour des algorithmes comme PCA.\n","    \"\"\"\n","    return Vectors.dense(arr) # Transforme le tableau en un vecteur dense Spark MLlib\n","\n","\n","\n","def vector_to_array(vector):\n","    \"\"\"\n","    Convertit un vecteur Spark en une liste (array) Python.\n","\n","    Args:\n","    vector (pyspark.ml.linalg.Vector): Un vecteur dense ou creux Spark.\n","\n","    Returns:\n","    list: Une liste Python contenant les valeurs du vecteur. Renvoie None si l'entrée n'est pas un vecteur.\n","    \"\"\"\n","\n","    return vector.toArray().tolist() if isinstance(vector, Vector) else None # Vérifie si l'entrée est un vecteur, le convertit en liste si c'est le cas"]},{"cell_type":"markdown","id":"5b9d7794","metadata":{},"source":["## Préparation à la réduction de dimension"]},{"cell_type":"code","execution_count":null,"id":"5b04f973","metadata":{"trusted":true},"outputs":[],"source":["# Conversion des arrays en vecteurs denses Spark pour être compatibles avec Spark MLlib\n","array_to_vector_udf = F.udf(convert_to_vector, VectorUDT())\n","\n","# Transformation du DataFrame pour ajouter une colonne de vecteurs de features\n","data_features_vectorized = data_features.withColumn(\"features_vector\", array_to_vector_udf(\"features\"))\n","\n","# Application de l'ACP dans un contexte distribué Spark\n","# k=1280 signifie qu'on réduit les dimensions des features à 1280 composantes\n","pca_spark = SparkPCA(k=1280, inputCol=\"features_vector\", outputCol=\"pca_features_vectors\")\n","pca_model = pca_spark.fit(data_features_vectorized)                 # Entraîne le modèle PCA sur les données vectorisées\n","pca_spark_result = pca_model.transform(data_features_vectorized)    # Applique la transformation PCA pour réduire les dimensions\n","\n","# Récupère la variance expliquée par chaque composante principale après l'application de la PCA\n","explained_variance = pca_model.explainedVariance.toArray()\n","\n","# Calcule la variance cumulée pour chaque composante. Cela permet de savoir combien de composantes\n","# sont nécessaires pour capturer un certain pourcentage de la variance totale des données.\n","cumulative_explained_variance = explained_variance.cumsum()\n","\n","# Trouve le nombre minimal de composantes qui cumulent au moins 80% de la variance totale\n","# La méthode 'argmax()' retourne l'indice de la première valeur supérieure ou égale à 0.8,\n","# donc on ajoute +1 pour obtenir le nombre de composantes nécessaires.\n","optimal_components_number = (cumulative_explained_variance >= 0.8).argmax() + 1\n","\n","print('-' * 80)\n","print(f\"Nombre de composantes necessaires pour expliquer 80% de la variance : {optimal_components_number}\")\n","print('-' * 80)\n","\n","# Crée une UDF pour convertir un vecteur Spark en un tableau Python\n","vector_to_array_udf = F.udf(vector_to_array, ArrayType(DoubleType()))\n","\n","# Ajoute une nouvelle colonne \"pca_array\" au DataFrame, qui contient les résultats PCA sous forme d'array\n","# Chaque vecteur PCA est converti en un array grâce à la fonction UDF définie ci-dessus.\n","pca_spark_result_array = pca_spark_result.withColumn(\"pca_array\", vector_to_array_udf(\"pca_features_vectors\"))\n","\n","pca_spark_result_array.printSchema()\n","\n","# Sélectionne les composantes qui permettent d'avoir au moins 80% de la variance\n","# Chaque composante a un alias (un nom unique \"F1\", \"F2\", etc...)\n","pca_cols = [F.col(\"pca_array\")[i].alias(f\"F {i + 1}\") for i in range(optimal_components_number)]\n","\n","# Crée un nouveau DataFrame avec les composantes sélectionnées et conserve la colonne \"label\"\n","data_extracted_pca = pca_spark_result_array.select(*pca_cols, \"label\")"]},{"cell_type":"markdown","id":"22ff10f2","metadata":{},"source":["## Exécution de la réduction de dimension et sauvegarde des résultats"]},{"cell_type":"code","execution_count":null,"id":"89866f03","metadata":{"trusted":true},"outputs":[],"source":["# Convertit le DataFrame Spark en DataFrame Pandas, ceci déclenche l'exécution des transformations Spark\n","# Cette action collecte toutes les données réparties sur le cluster pour les ramener en mémoire locale sous forme de DataFrame Pandas\n","data_extracted_pca_pandas = data_extracted_pca.toPandas()\n","\n","# Sauvegarde les résultats des composantes PCA dans un fichier CSV pour une utilisation future.\n","data_extracted_pca_pandas.to_csv('s3://ocr-p9/results_csv/pca_features.csv', index=False)"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"432.4px"},"toc_section_display":true,"toc_window_display":true}},"nbformat":4,"nbformat_minor":5}
